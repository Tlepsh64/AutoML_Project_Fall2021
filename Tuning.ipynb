{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aca588ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Models to use\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# Importing the metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# For measuring the training time taken during the fit process\n",
    "import time\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a56747cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>lepton_pT</th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet1pt</th>\n",
       "      <th>jet1eta</th>\n",
       "      <th>jet1phi</th>\n",
       "      <th>jet1b-tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet4eta</th>\n",
       "      <th>jet4phi</th>\n",
       "      <th>jet4b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.595839</td>\n",
       "      <td>-0.607811</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>1.818450</td>\n",
       "      <td>-0.111906</td>\n",
       "      <td>0.847550</td>\n",
       "      <td>-0.566437</td>\n",
       "      <td>1.581239</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654227</td>\n",
       "      <td>-1.274345</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.823761</td>\n",
       "      <td>0.938191</td>\n",
       "      <td>0.971758</td>\n",
       "      <td>0.789176</td>\n",
       "      <td>0.430553</td>\n",
       "      <td>0.961357</td>\n",
       "      <td>0.957818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  lepton_pT  lepton_eta  lepton_phi  missing_energy_magnitude  \\\n",
       "0      1   0.907542    0.329147    0.359412                  1.497970   \n",
       "1      1   0.798835    1.470639   -1.635975                  0.453773   \n",
       "2      0   1.344385   -0.876626    0.935913                  1.992050   \n",
       "3      1   1.105009    0.321356    1.522401                  0.882808   \n",
       "4      0   1.595839   -0.607811    0.007075                  1.818450   \n",
       "\n",
       "   missing_energy_phi    jet1pt   jet1eta   jet1phi  jet1b-tag  ...   jet4eta  \\\n",
       "0           -0.313010  1.095531 -0.557525 -1.588230   2.173076  ... -1.138930   \n",
       "1            0.425629  1.104875  1.282322  1.381664   0.000000  ...  1.128848   \n",
       "2            0.882454  1.786066 -1.646778 -0.942383   0.000000  ... -0.678379   \n",
       "3           -1.205349  0.681466 -1.070464 -0.921871   0.000000  ... -0.373566   \n",
       "4           -0.111906  0.847550 -0.566437  1.581239   2.173076  ... -0.654227   \n",
       "\n",
       "    jet4phi  jet4b-tag      m_jj     m_jjj      m_lv     m_jlv      m_bb  \\\n",
       "0 -0.000819   0.000000  0.302220  0.833048  0.985700  0.978098  0.779732   \n",
       "1  0.900461   0.000000  0.909753  1.108330  0.985692  0.951331  0.803252   \n",
       "2 -1.360356   0.000000  0.946652  1.028704  0.998656  0.728281  0.869200   \n",
       "3  0.113041   0.000000  0.755856  1.361057  0.986610  0.838085  1.133295   \n",
       "4 -1.274345   3.101961  0.823761  0.938191  0.971758  0.789176  0.430553   \n",
       "\n",
       "      m_wbb    m_wwbb  \n",
       "0  0.992356  0.798343  \n",
       "1  0.865924  0.780118  \n",
       "2  1.026736  0.957904  \n",
       "3  0.872245  0.808487  \n",
       "4  0.961357  0.957818  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('higgs_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c7d67663",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('class', axis=1), df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1864)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=1864)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c25ac9",
   "metadata": {},
   "source": [
    "Scaling is surely an important part of the pipeline but I realized that I forgot doing it in the end. I will add several choices for scalers later on, but for now lets move to designing the search space.\n",
    "\n",
    "For the meaning of search functions such as hp.uniform, you can take a look at here: http://hyperopt.github.io/hyperopt/getting-started/search_spaces/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a0489a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_parameters = {\n",
    "    'l2_leaf_reg': hp.qloguniform('l2_leaf_reg', 0, 2, 1), # Coefficient of the regularizer for the cost function.\n",
    "    'learning_rate': hp.uniform('learning_rate', 1e-3, 5e-1), # Self-explanatory. I think this will be quite important since it affects overfitting\n",
    "    'max_depth': hp.choice('max_depth', [2, 3, 6, 8, 10]), # Tree depths\n",
    "    'num_trees': hp.choice('num_trees', [25, 50, 100]), # Number of trees to be grown\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1.0), # Fraction of randomly selected features to use at each split\n",
    "    'min_data_in_leaf': hp.choice('min_data_in_leaf', [2, 3, 4, 6, 8]), # Minimum data required in leaves for a split to occur\n",
    "    #'num_leaves': hp.choice('num_leaves', [4, 8, 16, 32]), # max number of leaves\n",
    "    'eval_metric': 'Accuracy',\n",
    "    'loss_function':'Logloss',\n",
    "    'random_seed': 1864\n",
    "}\n",
    "\n",
    "fit_parameters = {\n",
    "    'early_stopping_rounds': 10,\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "ctb_para = dict()\n",
    "ctb_para['clf_params'] = classifier_parameters\n",
    "ctb_para['fit_params'] = fit_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288530e7",
   "metadata": {},
   "source": [
    "Objective function to optimize. It is a bit wordy for now but defining it as a class rather than a function may come in handy later on, in case we add more boosting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "efcee33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class objective_fn(object):\n",
    "    \n",
    "    def __init__(self, X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.X_val   = X_val\n",
    "        self.X_test  = X_test\n",
    "        \n",
    "        self.y_train = y_train\n",
    "        self.y_val   = y_val\n",
    "        self.y_test  = y_test\n",
    "        \n",
    "    def process(self, fn_name, space, trials, algo, max_evals):\n",
    "        fn = getattr(self, fn_name)\n",
    "        try:\n",
    "            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n",
    "        except Exception as e:\n",
    "            return {'status': STATUS_FAIL,\n",
    "                    'exception': str(e)}\n",
    "        return result, trials\n",
    "    \n",
    "    def ctb_clf(self, para):\n",
    "        clf = cb.CatBoostClassifier(**para['clf_params'])\n",
    "        return self.train(clf, para)\n",
    "    \n",
    "    def train(self, clf, para):\n",
    "        clf.fit(self.X_train, self.y_train,\n",
    "                eval_set=[(self.X_train, self.y_train), (self.X_val, self.y_val)],\n",
    "                **para['fit_params'])\n",
    "        \n",
    "        preds = clf.predict(self.X_test)\n",
    "        acc = accuracy_score(self.y_test, preds)\n",
    "        return {'loss': -acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b2925a",
   "metadata": {},
   "source": [
    "We can now start the trials. Hyperopt tries to minimize the objective function value so the more negative the accuracy, the better. Playing with clf parameters or the value of maximum number of evaluations may give better results, but I'll keep the searching procedure not too expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "923366ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [06:44<00:00,  4.04s/trial, best loss: -0.7218765935747068]\n"
     ]
    }
   ],
   "source": [
    "obj = objective_fn(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "trials = Trials()\n",
    "start = time.time()\n",
    "best = obj.process(fn_name='ctb_clf', space=ctb_para, trials=trials, algo=tpe.suggest, max_evals=100)\n",
    "end = time.time()\n",
    "\n",
    "total_time = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "adbb9e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for the best CatBoost Model:\n",
      "\n",
      "colsample_bylevel : 0.6477787840875192\n",
      "depth : 5\n",
      "l2_leaf_reg : 6.0\n",
      "learning_rate : 0.1479219353409454\n",
      "min_data_in_leaf : 4\n",
      "num_trees : 2\n",
      "\n",
      "Time taken for HyperParam Search: 6.74 mins\n"
     ]
    }
   ],
   "source": [
    "print('Parameters for the best CatBoost Model:')\n",
    "print()\n",
    "for k,v in ctb_opt[0].items():\n",
    "    print(k, ':', v)\n",
    "print()\n",
    "print('Time taken for HyperParam Search:', round(total_time/60, 2), 'mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a580b",
   "metadata": {},
   "source": [
    "We have used the hp.choice function which returns the **index** of the best hyperparameter inside the given list of hyperparameters. So of course, the best hyperparameters for the number of trees is not 2, but 100.\n",
    "\n",
    "The list below returns the correct model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b9d508fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bylevel': 0.5740694476873252,\n",
       " 'eval_metric': 'Accuracy',\n",
       " 'l2_leaf_reg': 1.0,\n",
       " 'learning_rate': 0.2727180907413367,\n",
       " 'loss_function': 'Logloss',\n",
       " 'max_depth': 6,\n",
       " 'min_data_in_leaf': 8,\n",
       " 'num_trees': 100,\n",
       " 'random_seed': 1864}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = space_eval(ctb_para['clf_params'], trials.argmin)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c2c1050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.722233554309026\n"
     ]
    }
   ],
   "source": [
    "clf = cb.CatBoostClassifier(**best_params)\n",
    "\n",
    "clf.fit(X_train, y_train, verbose=False, early_stopping_rounds=10)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "32d4c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.save_model(\n",
    "    \"model.json\",\n",
    "    format=\"json\",\n",
    "    # pool=pool  # this parameter is required only for models with categorical features.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f4d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
