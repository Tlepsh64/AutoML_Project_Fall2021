# AutoML_Project_Fall2021

This is the roject repo for the Explainable and Automated Machine Learning course given at UniTartu. The term project's task was to trying out several baselines on a picked dataset and then improving the selected baseline/baselines by applying hyperparameter tuning & automated feature generation. The results were to be compared by statistical significance testing at the end to understand if improvements were by chance or not.

To this end, we picked the higgs dataset from UCI(https://archive.ics.uci.edu/ml/datasets/HIGGS). Yet the number of instances was 11 Million in UCI's repository, so we changed to using the version of the higgs dataset published by OpenML for tuning & feature generation(https://www.openml.org/d/42769). Then we tested our best models on the UCI's version to understand if they would also do well on the original dataset. Finally, we compared the results from our models by using McNemar test.

  - You can find the baselines in the Baselines.ipynb folder.
  - For getting the hyperparameter tuning results of Catboost(the best performing baseline), please check catboost_tuning.ipynb for the small dataset. You can find the tuned model's performance result in catboost_tuning_.ipynb file, which also includes some extra evaluations.
  - For the tuning results of XGB, please check xgb_tuning and xgb_tuning_2 ipynb files. Among the tuned XGB models, the best performing(can be found under the first trial in xgb_tuning) one on the smaller test set had a worse accuracy result on the original test set of UCI's than that of the another model we obtained after the 4th trial in xgb_tuning_2. We resorted to using the hyperparameters used by the latter for the McNemar test.
  - For checking the tuning results of LGBM, please have a look at optuna-tuning-lgbm.ipynb file. Its results in terms of accuracy wasn't promising(ie the tuned model's score was lower than LGBM baseline) so we didn't use it for the final comparison via McNemar test.
  - For the results of automated feature generation, you can check 'autofeat_match_derived_features.ipynb' and 'autofeat_training.py' files. In the first one, we tried to imitate the manually designed features of the higgs dataset by just using autofeat, and the framework was able to achieve that. The latter one is just a .py script we ran using the resource of a high performance computing center(HPC), where we tried to combine the tuned catboost model with automated feature engineering to improve the results obtained at the tuning stage. Unfortunately it crashed several times even on HPC, and the version we were able to run achieved lower accuracy value than that of tuned catboost only.
  - For the significance testing results, you can check 'Comparing XGB to CatBoost_McNemar Test.ipynb', 'Significance Testing For Catboost_Big Dataset.ipynb', 'Significance Testing for Catboost on Small Test Set.ipynb' and 'Significance Testing For XGBoost_Big Dataset.ipynb'. The names are self-explanatory.


The best result we obtained is by using the tuned XGBoost model only. A proper run of it with autofeat would have improved it maybe, but generating new features seems to be too impractical with higgs data, since autofeat failed even on the small dataset published by OpenML after running for several hours.
