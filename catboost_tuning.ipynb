{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "surgical-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Models to use\n",
    "#import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# Importing the metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# For measuring the training time taken during the fit process\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "third-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "directed-james",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>lepton_pT</th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet1pt</th>\n",
       "      <th>jet1eta</th>\n",
       "      <th>jet1phi</th>\n",
       "      <th>jet1b-tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet4eta</th>\n",
       "      <th>jet4phi</th>\n",
       "      <th>jet4b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.595839</td>\n",
       "      <td>-0.607811</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>1.818450</td>\n",
       "      <td>-0.111906</td>\n",
       "      <td>0.847550</td>\n",
       "      <td>-0.566437</td>\n",
       "      <td>1.581239</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654227</td>\n",
       "      <td>-1.274345</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.823761</td>\n",
       "      <td>0.938191</td>\n",
       "      <td>0.971758</td>\n",
       "      <td>0.789176</td>\n",
       "      <td>0.430553</td>\n",
       "      <td>0.961357</td>\n",
       "      <td>0.957818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  lepton_pT  lepton_eta  lepton_phi  missing_energy_magnitude  \\\n",
       "0      1   0.907542    0.329147    0.359412                  1.497970   \n",
       "1      1   0.798835    1.470639   -1.635975                  0.453773   \n",
       "2      0   1.344385   -0.876626    0.935913                  1.992050   \n",
       "3      1   1.105009    0.321356    1.522401                  0.882808   \n",
       "4      0   1.595839   -0.607811    0.007075                  1.818450   \n",
       "\n",
       "   missing_energy_phi    jet1pt   jet1eta   jet1phi  jet1b-tag  ...   jet4eta  \\\n",
       "0           -0.313010  1.095531 -0.557525 -1.588230   2.173076  ... -1.138930   \n",
       "1            0.425629  1.104875  1.282322  1.381664   0.000000  ...  1.128848   \n",
       "2            0.882454  1.786066 -1.646778 -0.942383   0.000000  ... -0.678379   \n",
       "3           -1.205349  0.681466 -1.070464 -0.921871   0.000000  ... -0.373566   \n",
       "4           -0.111906  0.847550 -0.566437  1.581239   2.173076  ... -0.654227   \n",
       "\n",
       "    jet4phi  jet4b-tag      m_jj     m_jjj      m_lv     m_jlv      m_bb  \\\n",
       "0 -0.000819   0.000000  0.302220  0.833048  0.985700  0.978098  0.779732   \n",
       "1  0.900461   0.000000  0.909753  1.108330  0.985692  0.951331  0.803252   \n",
       "2 -1.360356   0.000000  0.946652  1.028704  0.998656  0.728281  0.869200   \n",
       "3  0.113041   0.000000  0.755856  1.361057  0.986610  0.838085  1.133295   \n",
       "4 -1.274345   3.101961  0.823761  0.938191  0.971758  0.789176  0.430553   \n",
       "\n",
       "      m_wbb    m_wwbb  \n",
       "0  0.992356  0.798343  \n",
       "1  0.865924  0.780118  \n",
       "2  1.026736  0.957904  \n",
       "3  0.872245  0.808487  \n",
       "4  0.961357  0.957818  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('higgs_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cathedral-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('class', axis=1), df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1864)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "engaged-evidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train / test split size: train set size: 78439, test set size: 19610\n"
     ]
    }
   ],
   "source": [
    "print(f'Train / test split size: train set size: {X_train.shape[0]}, test set size: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-resident",
   "metadata": {},
   "source": [
    "## The run that have found first optimal params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "religious-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_fn(params):\n",
    "    clf = cb.CatBoostClassifier(**params['clf_params'], verbose=False, task_type=\"GPU\")\n",
    "    acc = cross_val_score(clf, X_train, y_train, scoring='accuracy').mean()\n",
    "    return {\"loss\": -acc, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-dakota",
   "metadata": {},
   "source": [
    "We have considered multiple search spaces in different configurations before finding the one, where we have found the parameters that have outperformed baseline. Some of the hyperparameters that we have also considered, but that are not present in the current search space: colsample_bylevel (not supported on GPU), min_data_in_leaf, num_leaves, num_trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "functioning-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_parameters = {\n",
    "    'l2_leaf_reg': hp.choice('l2_leaf_reg', [3,1,5,10,100]), \n",
    "    'learning_rate': hp.uniform('learning_rate', 1e-3, 5e-1),\n",
    "    'depth': hp.choice('depth', [6, 7, 8, 9, 10]),\n",
    "    'random_strength': hp.uniform('random_strength', 0.0, 100),\n",
    "    'border_count': hp.choice('border_count', [128, 254]),\n",
    "    'bagging_temperature': hp.uniform('bagging_temperature', 0.0, 100),\n",
    "    'eval_metric': 'Accuracy',\n",
    "    'loss_function':'Logloss',\n",
    "    'random_seed': 1864\n",
    "}\n",
    "\n",
    "fit_parameters = {\n",
    "    'early_stopping_rounds': 10,\n",
    "    'verbose': True\n",
    "}\n",
    "\n",
    "ctb_para = dict()\n",
    "ctb_para['clf_params'] = classifier_parameters\n",
    "ctb_para['fit_params'] = fit_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "decreased-butterfly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 0/200 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [2:25:32<00:00, 43.66s/trial, best loss: -0.7297772498897755]\n",
      "{'clf_params': {'bagging_temperature': 1.4426651823376004, 'border_count': 128, 'depth': 7, 'eval_metric': 'Accuracy', 'l2_leaf_reg': 10, 'learning_rate': 0.05212984419824801, 'loss_function': 'Logloss', 'random_seed': 1864, 'random_strength': 62.078606789809335}, 'fit_params': {'early_stopping_rounds': 10, 'verbose': True}}\n",
      "0.7308516063233045\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective_fn,\n",
    "    space = ctb_para, \n",
    "    algo=tpe.suggest, \n",
    "    max_evals=200, \n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_params = space_eval(ctb_para, best)\n",
    "print(best_params)\n",
    "\n",
    "clf = cb.CatBoostClassifier(**best_params['clf_params'])\n",
    "\n",
    "clf.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-williams",
   "metadata": {},
   "source": [
    "** ! We observe that sinc we pass dictionrary `ctb_para`, when we print `best_params` we also print `fit_params`, but we have not passed them to `cross_val_score` function during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-learning",
   "metadata": {},
   "source": [
    "Since the cells above have been copied from other notebook, we will explicitly specify the parameters during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sharp-philadelphia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the current optimal catboost model: 0.7308516063233045\n"
     ]
    }
   ],
   "source": [
    "clf = cb.CatBoostClassifier(\n",
    "    bagging_temperature = 1.4426651823376004,\n",
    "    border_count = 128,\n",
    "    depth = 7,\n",
    "    eval_metric = 'Accuracy',\n",
    "    l2_leaf_reg = 10,\n",
    "    learning_rate = 0.05212984419824801,\n",
    "    loss_function = 'Logloss',\n",
    "    random_seed = 1864,\n",
    "    random_strength = 62.078606789809335\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train, verbose=False)\n",
    "preds = clf.predict(X_test)\n",
    "print(f'Test accuracy of the current optimal catboost model: {accuracy_score(y_test, preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-passage",
   "metadata": {},
   "source": [
    "### Modified class definition (TO DO: pass `fit_params` to `cross_val_score` function and re-run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "billion-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class objective_fn(object):\n",
    "    \n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def process(self, fn_name, space, trials, algo, max_evals):\n",
    "        fn = getattr(self, fn_name)\n",
    "        try:\n",
    "            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n",
    "        except Exception as e:\n",
    "            return {'status': STATUS_FAIL,\n",
    "                    'exception': str(e)}\n",
    "        return result, trials\n",
    "    \n",
    "    def ctb_clf(self, params):\n",
    "        clf = cb.CatBoostClassifier(**params, verbose=False, task_type=\"GPU\")\n",
    "        acc = cross_val_score(clf, self.X_train, self.y_train, scoring='accuracy').mean()\n",
    "        return {\"loss\": -acc, \"status\": STATUS_OK}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "appropriate-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_parameters = {\n",
    "    'l2_leaf_reg': hp.choice('l2_leaf_reg', [3,1,5,10,100]), \n",
    "    'learning_rate': hp.uniform('learning_rate', 1e-3, 5e-1),\n",
    "    'depth': hp.choice('depth', [6, 7, 8, 9, 10]),\n",
    "    'random_strength': hp.uniform('random_strength', 0.0, 100),\n",
    "    'border_count': hp.choice('border_count', [128, 254]),\n",
    "    'bagging_temperature': hp.uniform('bagging_temperature', 0.0, 100),\n",
    "    'eval_metric': 'Accuracy',\n",
    "    'loss_function':'Logloss',\n",
    "    'random_seed': 1864\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "foster-boring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [4:48:17<00:00, 57.66s/trial, best loss: -0.7291015774648418]\n"
     ]
    }
   ],
   "source": [
    "obj = objective_fn(X_train, y_train)\n",
    "\n",
    "trials = Trials()\n",
    "start = time.time()\n",
    "best = obj.process(fn_name='ctb_clf', space=classifier_parameters, trials=trials, algo=tpe.suggest, max_evals=300)\n",
    "end = time.time()\n",
    "\n",
    "total_time = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "broke-madison",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_temperature': 0.2879213699517955,\n",
       " 'border_count': 128,\n",
       " 'depth': 10,\n",
       " 'eval_metric': 'Accuracy',\n",
       " 'l2_leaf_reg': 5,\n",
       " 'learning_rate': 0.031731849925222905,\n",
       " 'loss_function': 'Logloss',\n",
       " 'random_seed': 1864,\n",
       " 'random_strength': 59.7185747097968}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params = space_eval(classifier_parameters, best[0])\n",
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "reduced-blink",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the current optimal catboost model: 0.7316165221825599\n"
     ]
    }
   ],
   "source": [
    "clf = cb.CatBoostClassifier(\n",
    "    **opt_params\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train, verbose=False)\n",
    "preds = clf.predict(X_test)\n",
    "print(f'Test accuracy of the current optimal catboost model: {accuracy_score(y_test, preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-prototype",
   "metadata": {},
   "source": [
    "We observe the confirmation that `cross validation` in a more pessimistics estimation of the generalization error: for the previous optimal parameters the cross validated accuracy was `0.7297772498897755`, while the actual test accuracy: `0.7308516063233045`, at the same time the cross validated accuracy of our current optimal model was `0.7291015774648418` (which is lower), while the actual test accuracy is `0.7316165221825599`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
